(window.webpackJsonp=window.webpackJsonp||[]).push([[69],{610:function(t,s,a){"use strict";a.r(s);var n=a(6),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("div",{staticClass:"custom-block warning"},[a("h3",{attrs:{id:"概念"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#概念"}}),t._v(" 概念")]),t._v(" "),a("p",[a("strong",[t._v("组件")])]),t._v(" "),a("p",[t._v("渲染器、解析器")]),t._v(" "),a("hr"),t._v(" "),a("ul",[a("li",[a("p",[t._v("RESTful规范")])]),t._v(" "),a("li",[a("p",[t._v("DRF序列化 | 反序列化")])]),t._v(" "),a("li",[a("p",[t._v("DRF视图 | 路由")])]),t._v(" "),a("li",[a("p",[t._v("DRF版本")])]),t._v(" "),a("li",[a("p",[t._v("DRF认证")])]),t._v(" "),a("li",[a("p",[t._v("权限")])]),t._v(" "),a("li",[a("p",[t._v("频率")])]),t._v(" "),a("li",[a("p",[t._v("分页")])])])]),t._v(" "),a("h3",{attrs:{id:"restful规范"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#restful规范"}}),t._v(" RESTful规范")]),t._v(" "),a("p",[t._v("在web中，只要有被引用的必要都叫资源。")]),t._v(" "),a("p",[t._v("每个URI代表一个资源，独一无二的。")]),t._v(" "),a("p",[t._v("客户端通过HTTP的方法，对服务器端资源进行操作；")]),t._v(" "),a("p",[t._v("客户端和服务器之间，传递这种资源的某种表现层；")]),t._v(" "),a("p",[t._v('通过超链接的指引，实现"表现层状态转移"。')]),t._v(" "),a("blockquote",[a("p",[t._v("规范")])]),t._v(" "),a("ol",[a("li",[a("p",[t._v("面向资源编程\n　　每个URL代表一种资源，URL中尽量不要用动词，要用名词。")])]),t._v(" "),a("li",[a("p",[t._v("根据method不同，进行不同的操作\n　　GET/POST/PUT/DELETE/PATCH")])]),t._v(" "),a("li",[a("p",[t._v("在URL中体现版本")])])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("v1"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("book\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("v2"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("book\n")])])]),a("ol",{attrs:{start:"4"}},[a("li",[t._v("URL过滤条件")])]),t._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[t._v("/v1/book?page"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\n")])])]),a("ol",{attrs:{start:"5"}},[a("li",[t._v("响应状态码")])]),t._v(" "),a("h2",{attrs:{id:"序列化与反序列化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#序列化与反序列化"}}),t._v(" 序列化与反序列化")]),t._v(" "),a("p",[a("strong",[t._v("序列化")]),t._v("是：数据对象从数据库中查出，通过instance传入序列化器中，必须通过data属性才能将序列化后的数据传给前端，不能直接传序列化对象")]),t._v(" "),a("p",[a("strong",[t._v("反序列化")]),t._v("是：数据是通过request.data从前端获取到数据，通过data传入序列化器中进行校验，保存到数据库中")]),t._v(" "),a("blockquote",[a("p",[t._v("使用的两个阶段")])]),t._v(" "),a("p",[t._v("客户端请求时：使用序列化器可以完成对数据的"),a("u",[a("strong",[t._v("反序列化")])]),t._v("（就是前段往后端传递数据，反序列化之后保存数据）")]),t._v(" "),a("p",[t._v("服务器响应时，使用序列化器可以完成对数据的"),a("u",[a("strong",[t._v("序列化")])]),t._v("（服务器取出数据，序列化之后往前段发送展示）")]),t._v(" "),a("h3",{attrs:{id:"反序列化流程"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#反序列化流程"}}),t._v(" 反序列化流程")]),t._v(" "),a("p",[t._v("先验证后保存")]),t._v(" "),a("p",[a("strong",[t._v("数据验证")])]),t._v(" "),a("p",[t._v("使用序列化器反序列化。需要对数据进行校验后，才能获取验证成功的数据然后保存模型类对象")]),t._v(" "),a("p",[t._v("在获取反序列化数据前。必须调用"),a("code",[t._v("is_valid()")]),t._v(" 进行验证。成功True 失败False")]),t._v(" "),a("ul",[a("li",[t._v("通过可以通过"),a("code",[t._v("对象.validated_data")]),t._v("获取数据")]),t._v(" "),a("li",[t._v("失败可以通过"),a("code",[t._v("对象.errors")]),t._v(" 获取错误信息。")])]),t._v(" "),a("p",[a("strong",[t._v("保存数据")])]),t._v(" "),a("p",[t._v("序列化类中必须重写create方法用于新增，重写update方法是修改")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 例子")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("）设置必填与选填序列化字段，设置校验规则\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("）为需要额外校验的字段提供局部钩子函数，如果该字段不入库，且不参与全局钩子校验，可以将值取出校验 pop\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("）为有联合关系的字段们提供全局钩子函数，如果某些字段不入库，可以将值取出校验\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("）必须重写create方法，完成校验通过的数据入库工作，得到新增的对象\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("UserDeserializer")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("serializers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Serializer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1) 哪些字段必须反序列化")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2) 字段都有哪些安全校验")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3) 哪些字段需要额外提供校验  钩子函数")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4) 哪些字段间存在联合校验")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 注：反序列化字段都是用来入库的，不会出现自定义方法属性，会出现可以设置校验规则的自定义属性,不入数据库的")]),t._v("\n    name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" serializers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CharField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        max_length"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        min_length"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        error_messages"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'max_length'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'太长'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'min_length'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'太短'")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    pwd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" serializers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CharField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    phone "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" serializers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CharField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("required"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    sex "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" serializers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("IntegerField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("required"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 自定义有校验规则的反序列化字段,例如确认密码字段re_pwd")]),t._v("\n    re_pwd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" serializers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CharField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("required"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 局部钩子：validate_要校验的字段名(self, 当前要校验字段的值)")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 校验规则：校验通过返回原值，校验失败，抛出异常")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("validate_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" value"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'g'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" value"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lower"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 名字中不能出现g")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("raise")]),t._v(" exceptions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ValidationError"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'名字非法，是个鸡贼！'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" value\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 全局钩子：validate(self, 通过系统与局部钩子校验之后的所有数据)")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 校验规则：校验通过返回原值，校验失败，抛出异常")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("validate")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" attrs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#attrs是字典格式")]),t._v("\n        pwd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" attrs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pwd'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        re_pwd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" attrs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pop"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'re_pwd'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("     "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#因为re_pwd不需要入数据库，所以在全局钩子校验中删除掉这个字段")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" pwd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),t._v(" re_pwd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("raise")]),t._v(" exceptions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ValidationError"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pwd&re_pwd'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'两次密码不一致'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" attrs\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 要完成新增，必须自己重写 create 方法，validated_data是校验的数据")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("create")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" validated_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 尽量在所有校验规则完毕之后，数据可以直接入库")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" models"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("User"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("objects"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("create"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("validated_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("h3",{attrs:{id:"自定义序列化属性"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#自定义序列化属性"}}),t._v(" 自定义序列化属性")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\ngender "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" serializers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("SerializerMethodField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_gender")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" obj"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# choice类型的解释型值 get_字段_display() 来访问")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" obj"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_sex_display"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n")])])]),a("h3",{attrs:{id:"嵌套序列化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#嵌套序列化"}}),t._v(" 嵌套序列化")]),t._v(" "),a("p",[t._v("正常情况下，我们不会有简单一个数据表来序列化。通常是多个表序列化。")]),t._v(" "),a("p",[t._v("那我们就需要考虑多表如何嵌套序列化")]),t._v(" "),a("p",[a("strong",[t._v("三种方法")])]),t._v(" "),a("p",[t._v("直接在models中定义方法。 然后我们在序列化类中用fields引用即可。")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# models.py")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Book")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("models"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@property")]),t._v(" \n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("publish_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 出版社名称 (反向查询)")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("publish"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name\n\n    "),a("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@property")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("author_list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取作者列表 (反向查询所有)")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("authors"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("all")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Serializer.py")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("BookSerializer")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("serializers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ModelSerializer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Meta")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Book\n        fields "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'name'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'price'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'publish_name'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'author_list'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'authors'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("第二种方法：引用其他模型")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("div",{staticClass:"highlight-lines"},[a("br"),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("br"),a("br"),a("br"),a("br"),a("br")]),a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Serializer.py")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("BookSerializer")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("serializers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ModelSerializer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    authors "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AuthorSerializer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("read_only"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" many"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Meta")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Book\n        fields "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'authors'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("第三种方法： 自定义字段")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Serializer.py")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("BookSerializer")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("serializers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ModelSerializer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    authors "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Halo'")]),t._v("\n    \n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Meta")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Book\n        fields "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'__all__'")]),t._v("\n")])])]),a("hr"),t._v(" "),a("p",[t._v("序列化时，将模型类传入"),a("code",[t._v("instance")]),t._v("参数")]),t._v(" "),a("p",[t._v("反序列化时，将要被反序列化的数据传入"),a("code",[t._v("data")]),t._v("参数")]),t._v(" "),a("p",[t._v("除了instance和data参数。在构造Serializer对象。还可以通过context参数添加额外数据。")]),t._v(" "),a("h3",{attrs:{id:"验证"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#验证"}}),t._v(" 验证")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("单个字段验证")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" validate_字段"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" value"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" value\n")])])])]),t._v(" "),a("li",[a("p",[t._v("验证器"),a("code",[t._v("validators")])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("validate")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" attrs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#attrs是字典格式")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" attrs\n")])])])])]),t._v(" "),a("h2",{attrs:{id:"视图"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#视图"}}),t._v(" 视图")]),t._v(" "),a("p",[t._v("django中写CBV的时候继承的是View，rest_framework继承的是APIView 。")]),t._v(" "),a("p",[a("img",{attrs:{src:"http://alicdn.itaolaity.com/img/20200608181003.png",alt:""}})]),t._v(" "),a("h2",{attrs:{id:"路由"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#路由"}}),t._v(" 路由")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" rest_framework"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("routers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" DefaultRouter\n\nrouter "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DefaultRouter"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nrouter"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("register"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('r"book"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" BookView"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nurlpatterns "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("router"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("urls\n")])])]),a("h2",{attrs:{id:"解析器"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#解析器"}}),t._v(" 解析器")]),t._v(" "),a("p",[t._v("解析器的作用就是服务端接收客户端传过来的数据，把数据解析成自己想要的数据类型的过程。")]),t._v(" "),a("p",[t._v("本质就是对请求体中的数据进行解析。")]),t._v(" "),a("p",[a("strong",[t._v("局部使用")])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("parser_classes "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("JSONParser"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),a("p",[a("strong",[t._v("全局使用")])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# rest_framework.parsers.BaseParser")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#默认的解析器")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'DEFAULT_PARSER_CLASSES'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rest_framework.parsers.JSONParser'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rest_framework.parsers.FormParser'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rest_framework.parsers.MultiPartParser'")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[a("img",{attrs:{src:"https://cdn.nlark.com/yuque/0/2020/png/326318/1591100014871-8250b9fe-1eb7-412c-acf0-3ca957bff9ed.png",alt:"image.png"}}),a("img",{attrs:{src:"https://cdn.nlark.com/yuque/0/2020/png/326318/1591100014871-8250b9fe-1eb7-412c-acf0-3ca957bff9ed.png",alt:"img"}})]),t._v(" "),a("h2",{attrs:{id:"渲染器"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#渲染器"}}),t._v(" 渲染器")]),t._v(" "),a("p",[t._v("默认的渲染器")]),t._v(" "),a("p",[a("code",[t._v("rest_framework.settings.py")])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("DEFAULTS "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 渲染器")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'DEFAULT_RENDERER_CLASSES'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rest_framework.renderers.JSONRenderer'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rest_framework.renderers.BrowsableAPIRenderer'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n")])])]),a("h2",{attrs:{id:"参考"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考"}}),t._v(" 参考")]),t._v(" "),a("p",[t._v("https://www.cnblogs.com/study-learning/p/10126773.html")]),t._v(" "),a("p",[t._v("https://www.cnblogs.com/wangcuican/p/11748025.html")])])}),[],!1,null,null,null);s.default=e.exports}}]);